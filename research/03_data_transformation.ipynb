{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g:\\\\success_analytics_courses\\\\internship_project\\\\pulsar_project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step - 1 : config.yaml updated\n",
    "#step - 2 : constant file updated\n",
    "#step - 3 : entity\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfiguration:\n",
    "\n",
    "    root_dir_name: Path\n",
    "    dataset_download_url: str\n",
    "    zip_data_dir_name: Path\n",
    "    unzip_data_dir_name: Path\n",
    "    \n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfiguration:\n",
    "\n",
    "    validated_root_dir_name: Path\n",
    "    validated_train_dir: Path\n",
    "    validated_test_dir: Path\n",
    "    validated_status_report_file_name: str\n",
    "    validated_required_files:list\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfiguration:\n",
    "\n",
    "    transformed_root_dir_name: Path\n",
    "    transformed_train_dir: Path\n",
    "    transformed_test_dir: Path\n",
    "    transformed_industrial_data_dir: Path\n",
    "    transformed_preprocess_dir: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step - 4 : configuration manager in src config\n",
    "from pulsarclassification.constants import *\n",
    "from pulsarclassification.logging import logging\n",
    "from pulsarclassification.utils.common import read_yaml,create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "\n",
    "    def __init__(self, config_file_path: str = CONFIG_FILE_PATH):\n",
    "        \n",
    "        try:\n",
    "            self.config = read_yaml(CONFIG_FILE_PATH)\n",
    "            create_directories(self.config.artifacts_dir_name)\n",
    "            logging.info(f\" Artifacts directory created at : {self.config.artifacts_dir_name} \")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfiguration:\n",
    "\n",
    "        try:\n",
    "            artifact_dir = self.config.artifacts_dir_name\n",
    "            config = self.config.data_ingestion_config\n",
    "\n",
    "            data_ingestion_dir = os.path.join(artifact_dir,config.root_dir_name)\n",
    "            create_directories(data_ingestion_dir)\n",
    "\n",
    "            raw_data_dir = os.path.join(data_ingestion_dir,config.zip_data_dir_name)\n",
    "            create_directories(raw_data_dir)\n",
    "\n",
    "            ingested_csv_data_dir = os.path.join(data_ingestion_dir,config.unzip_data_dir_name)\n",
    "            create_directories(ingested_csv_data_dir)\n",
    "\n",
    "            data_ingestion_config = DataIngestionConfiguration(\n",
    "                root_dir_name  = config.root_dir_name,\n",
    "                dataset_download_url = config.dataset_download_url,\n",
    "                zip_data_dir_name = raw_data_dir,\n",
    "                unzip_data_dir_name = ingested_csv_data_dir\n",
    "            )\n",
    "\n",
    "            logging.info(f\" Data ingestion configuration: {data_ingestion_config}\")\n",
    "\n",
    "            return data_ingestion_config\n",
    "    \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def get_data_validation_configuration(self) -> DataValidationConfiguration:\n",
    "\n",
    "        try:\n",
    "            artifact_dir = self.config.artifacts_dir_name\n",
    "            config = self.config.data_validation_config\n",
    "\n",
    "            data_validation_dir = os.path.join(artifact_dir,config.validated_root_dir_name)\n",
    "            create_directories(data_validation_dir)\n",
    "\n",
    "            data_validation_train_dir = os.path.join(data_validation_dir,config.validated_train_dir)\n",
    "            create_directories(data_validation_train_dir)\n",
    "\n",
    "            data_validation_test_dir = os.path.join(data_validation_dir,config.validated_test_dir)\n",
    "            create_directories(data_validation_test_dir)\n",
    "\n",
    "            data_validation_config = DataValidationConfiguration(\n",
    "                validated_root_dir_name  = config.validated_root_dir_name,\n",
    "                validated_train_dir = data_validation_train_dir,\n",
    "                validated_test_dir = data_validation_test_dir,\n",
    "                validated_status_report_file_name = os.path.join(data_validation_dir,config.validated_status_report_file_name),\n",
    "                validated_required_files = config.validated_required_files\n",
    "            )\n",
    "\n",
    "            logging.info(f\" Data validation configuration: {data_validation_config}\")\n",
    "\n",
    "            return data_validation_config\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def get_data_transformation_configuration(self) -> DataTransformationConfiguration:\n",
    "\n",
    "        try:\n",
    "            artifact_dir = self.config.artifacts_dir_name\n",
    "            config = self.config.data_transformation_config\n",
    "\n",
    "            data_transformation_dir = os.path.join(artifact_dir,config.transformed_root_dir_name)\n",
    "            create_directories(data_transformation_dir)\n",
    "\n",
    "            data_transformation_train_dir = os.path.join(data_transformation_dir,config.transformed_train_dir)\n",
    "            create_directories(data_transformation_train_dir)\n",
    "\n",
    "            data_transformation_test_dir = os.path.join(data_transformation_dir,config.transformed_test_dir)\n",
    "            create_directories(data_transformation_test_dir)\n",
    "\n",
    "            data_transformation_industrial_data_dir = os.path.join(data_transformation_dir,config.transformed_industrial_data_dir)\n",
    "            create_directories(data_transformation_industrial_data_dir)\n",
    "\n",
    "            data_transformation_preprocess_data_dir = os.path.join(data_transformation_dir,config.transformed_preprocess_dir)\n",
    "            create_directories(data_transformation_preprocess_data_dir)\n",
    "\n",
    "\n",
    "            data_transformation_config = DataTransformationConfiguration(\n",
    "                transformed_root_dir_name = data_transformation_dir,\n",
    "                transformed_train_dir = data_transformation_train_dir,\n",
    "                transformed_test_dir =  data_transformation_test_dir,\n",
    "                transformed_industrial_data_dir =  data_transformation_industrial_data_dir,\n",
    "                transformed_preprocess_dir = data_transformation_preprocess_data_dir\n",
    "            )\n",
    "\n",
    "            logging.info(f\" Data transformation configuration: {data_transformation_config}\")\n",
    "\n",
    "            return data_transformation_config\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage - 6 : updating components\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from pulsarclassification.logging import logging\n",
    "from pulsarclassification.constants import *\n",
    "from pulsarclassification.utils.common import read_yaml,create_directories,get_file_size\n",
    "from pulsarclassification.entity import DataIngestionConfiguration,DataValidationConfiguration\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, \n",
    "                 validation_config:DataValidationConfiguration,\n",
    "                 transformation_config: DataTransformationConfiguration):\n",
    "\n",
    "        try:\n",
    "            self.validation_config = validation_config\n",
    "            self.transformation_config = transformation_config\n",
    "            self.schema = read_yaml(SCHEMA_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            raise e \n",
    "        \n",
    "    def file_transformation_saving(self):\n",
    "        try:\n",
    "            model_data_file_path = os.path.join(self.validation_config.validated_train_dir,VALIDATED_DATA_FILE_NAME_FOR_MODEL_TRAIN)\n",
    "            industrial_data_file = os.path.join(self.validation_config.validated_test_dir,VALIDATED_INDUSTRIALDATA_FILE_NAME)\n",
    "            \n",
    "            model_data = pd.read_csv(model_data_file_path)\n",
    "            industrial_data = pd.read_csv(industrial_data_file)\n",
    "\n",
    "            features = self.schema.numeriacl_columns.split(\" \")\n",
    "            features.remove(\"id\")\n",
    "            model_data = model_data[features]\n",
    "            features.remove(self.schema.target_column)\n",
    "            industrial_data = industrial_data[features]\n",
    "\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "            model_train_set = None\n",
    "            model_test_set = None\n",
    "\n",
    "            for train_index,test_index in sss.split(model_data,model_data['Class']):\n",
    "                model_train_set = model_data.loc[train_index]\n",
    "                model_test_set = model_data.loc[test_index]\n",
    "\n",
    "            model_train_set.reset_index(drop=True,inplace=True)\n",
    "            model_test_set.reset_index(drop=True,inplace=True)\n",
    "\n",
    "            model_train_set.to_csv(os.path.join(self.transformation_config.transformed_train_dir,\n",
    "                                                TRANSFORMED_MODEL_TRAIN_FILE_NAME),index=False)\n",
    "            logging.info(f\"{TRANSFORMED_MODEL_TRAIN_FILE_NAME} saved in {self.transformation_config.transformed_train_dir}\")\n",
    "            model_test_set.to_csv(os.path.join(self.transformation_config.transformed_test_dir,\n",
    "                                                TRANSFORMED_MODEL_TEST_FILE_NAME),index=False)\n",
    "            logging.info(f\"{TRANSFORMED_MODEL_TEST_FILE_NAME} saved in {self.transformation_config.transformed_test_dir}\")\n",
    "            industrial_data.to_csv(os.path.join(self.transformation_config.transformed_industrial_data_dir,\n",
    "                                                TRANSFORMED_INDUSTRIALDATA_FILE_NAME),index=False)\n",
    "            logging.info(f\"{TRANSFORMED_INDUSTRIALDATA_FILE_NAME} saved in {self.transformation_config.transformed_industrial_data_dir}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24-08-2023 07:37:08: INFO: common:  yaml file from this path config\\config.yaml read succesfully]\n",
      "[24-08-2023 07:37:08: INFO: common:  Directory already present: artifacts ]\n",
      "[24-08-2023 07:37:08: INFO: 4025342101:  Artifacts directory created at : artifacts ]\n",
      "[24-08-2023 07:37:08: INFO: common:  Directory already present: artifacts\\data_validation ]\n",
      "[24-08-2023 07:37:08: INFO: common:  Directory already present: artifacts\\data_validation\\training_data_for_model ]\n",
      "[24-08-2023 07:37:08: INFO: common:  Directory already present: artifacts\\data_validation\\industrial_test_data ]\n",
      "[24-08-2023 07:37:08: INFO: 4025342101:  Data validation configuration: DataValidationConfiguration(validated_root_dir_name='data_validation', validated_train_dir='artifacts\\\\data_validation\\\\training_data_for_model', validated_test_dir='artifacts\\\\data_validation\\\\industrial_test_data', validated_status_report_file_name='artifacts\\\\data_validation\\\\status.txt', validated_required_files=BoxList(['train', 'test', 'sample_submission']))]\n",
      "[24-08-2023 07:37:08: INFO: common:  Directory created in this: artifacts\\data_transformation ]\n",
      "[24-08-2023 07:37:08: INFO: common:  Directory created in this: artifacts\\data_transformation\\training_data_for_model ]\n",
      "[24-08-2023 07:37:08: INFO: common:  Directory created in this: artifacts\\data_transformation\\test_data_for_model ]\n",
      "[24-08-2023 07:37:08: INFO: common:  Directory created in this: artifacts\\data_transformation\\industrial_test_data ]\n",
      "[24-08-2023 07:37:08: INFO: common:  Directory created in this: artifacts\\data_transformation\\preprocessed_pickle_file ]\n",
      "[24-08-2023 07:37:08: INFO: 4025342101:  Data transformation configuration: DataTransformationConfiguration(transformed_root_dir_name='artifacts\\\\data_transformation', transformed_train_dir='artifacts\\\\data_transformation\\\\training_data_for_model', transformed_test_dir='artifacts\\\\data_transformation\\\\test_data_for_model', transformed_industrial_data_dir='artifacts\\\\data_transformation\\\\industrial_test_data', transformed_preprocess_dir='artifacts\\\\data_transformation\\\\preprocessed_pickle_file')]\n",
      "[24-08-2023 07:37:08: INFO: common:  yaml file from this path config\\schema.yaml read succesfully]\n",
      "[24-08-2023 07:37:11: INFO: 3923247658: pulsar_train_data.csv saved in artifacts\\data_transformation\\training_data_for_model]\n",
      "[24-08-2023 07:37:11: INFO: 3923247658: pulsar_test_data.csv saved in artifacts\\data_transformation\\test_data_for_model]\n",
      "[24-08-2023 07:37:12: INFO: 3923247658: Industrial_pulsar_data.csv saved in artifacts\\data_transformation\\industrial_test_data]\n"
     ]
    }
   ],
   "source": [
    "#from pulsarclassification.config.configuration import ConfigurationManager\n",
    "#from pulsarclassification.components.data_validation import DataValidation\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_validation_config = config.get_data_validation_configuration()\n",
    "    data_transformation_config = config.get_data_transformation_configuration()\n",
    "    data_transformation = DataTransformation(validation_config=data_validation_config,\n",
    "                                     transformation_config=data_transformation_config)\n",
    "    data_transformation.file_transformation_saving()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
