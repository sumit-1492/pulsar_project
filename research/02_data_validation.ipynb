{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g:\\\\success_analytics_courses\\\\internship_project\\\\pulsar_project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step - 1 : config.yaml updated\n",
    "#step - 2 : constant file updated\n",
    "#step - 3 : entity\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfiguration:\n",
    "\n",
    "    root_dir_name: Path\n",
    "    dataset_download_url: str\n",
    "    zip_data_dir_name: Path\n",
    "    unzip_data_dir_name: Path\n",
    "    \n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfiguration:\n",
    "\n",
    "    validated_root_dir_name: Path\n",
    "    validated_train_dir: Path\n",
    "    validated_test_dir: Path\n",
    "    validated_status_report_file_name: str\n",
    "    validated_required_files:list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step - 4 : configuration manager in src config\n",
    "from pulsarclassification.constants import *\n",
    "from pulsarclassification.logging import logging\n",
    "from pulsarclassification.utils.common import read_yaml,create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "\n",
    "    def __init__(self, config_file_path: str = CONFIG_FILE_PATH):\n",
    "        \n",
    "        try:\n",
    "            self.config = read_yaml(CONFIG_FILE_PATH)\n",
    "            create_directories(self.config.artifacts_dir_name)\n",
    "            logging.info(f\" Artifacts directory created at : {self.config.artifacts_dir_name} \")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfiguration:\n",
    "\n",
    "        try:\n",
    "            artifact_dir = self.config.artifacts_dir_name\n",
    "            config = self.config.data_ingestion_config\n",
    "\n",
    "            data_ingestion_dir = os.path.join(artifact_dir,config.root_dir_name)\n",
    "            create_directories(data_ingestion_dir)\n",
    "\n",
    "            raw_data_dir = os.path.join(data_ingestion_dir,config.zip_data_dir_name)\n",
    "            create_directories(raw_data_dir)\n",
    "\n",
    "            ingested_csv_data_dir = os.path.join(data_ingestion_dir,config.unzip_data_dir_name)\n",
    "            create_directories(ingested_csv_data_dir)\n",
    "\n",
    "            data_ingestion_config = DataIngestionConfiguration(\n",
    "                root_dir_name  = config.root_dir_name,\n",
    "                dataset_download_url = config.dataset_download_url,\n",
    "                zip_data_dir_name = raw_data_dir,\n",
    "                unzip_data_dir_name = ingested_csv_data_dir\n",
    "            )\n",
    "\n",
    "            logging.info(f\" Data ingestion configuration: {data_ingestion_config}\")\n",
    "\n",
    "            return data_ingestion_config\n",
    "    \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def get_data_validation_configuration(self) -> DataValidationConfiguration:\n",
    "\n",
    "        try:\n",
    "            artifact_dir = self.config.artifacts_dir_name\n",
    "            config = self.config.data_validation_config\n",
    "\n",
    "            data_validation_dir = os.path.join(artifact_dir,config.validated_root_dir_name)\n",
    "            create_directories(data_validation_dir)\n",
    "\n",
    "            data_validation_train_dir = os.path.join(data_validation_dir,config.validated_train_dir)\n",
    "            create_directories(data_validation_train_dir)\n",
    "\n",
    "            data_validation_test_dir = os.path.join(data_validation_dir,config.validated_test_dir)\n",
    "            create_directories(data_validation_test_dir)\n",
    "\n",
    "            data_validation_config = DataValidationConfiguration(\n",
    "                validated_root_dir_name  = config.validated_root_dir_name,\n",
    "                validated_train_dir = data_validation_train_dir,\n",
    "                validated_test_dir = data_validation_test_dir,\n",
    "                validated_status_report_file_name = os.path.join(data_validation_dir,config.validated_status_report_file_name),\n",
    "                validated_required_files = config.validated_required_files\n",
    "            )\n",
    "\n",
    "            logging.info(f\" Data validation configuration: {data_validation_config}\")\n",
    "\n",
    "            return data_validation_config\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage - 6 : updating components\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pulsarclassification.logging import logging\n",
    "from pulsarclassification.constants import *\n",
    "from pulsarclassification.utils.common import read_yaml,create_directories,get_file_size\n",
    "from pulsarclassification.entity import DataIngestionConfiguration,DataValidationConfiguration\n",
    "\n",
    "class DataValidation:\n",
    "    def __init__(self, ingestion_config : DataIngestionConfiguration,\n",
    "                 validation_config:DataValidationConfiguration):\n",
    "\n",
    "        try:\n",
    "            self.ingestion_config = ingestion_config\n",
    "            self.validation_config = validation_config\n",
    "            self.schema = read_yaml(SCHEMA_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            raise e \n",
    "        \n",
    "    def file_exist_validation(self):\n",
    "        try:\n",
    "            file_exist_status = None\n",
    "            all_files = os.listdir(self.ingestion_config.unzip_data_dir_name)\n",
    "            with open(self.validation_config.validated_status_report_file_name,'w') as f:\n",
    "                f.write(f\">>>>>>>>>>>>file exist validation<<<<<<<<<<<<<\\n\\n\")\n",
    "                for file in all_files:\n",
    "                    if file not in all_files:\n",
    "                        file_exist_status = False\n",
    "                        f.write(f\"Validation status: {file_exist_status}------->{file} not present\\n\\n\")\n",
    "                    else:\n",
    "                        file_exist_status = True\n",
    "                        f.write(f\"Validation status: {file_exist_status}------->{file} is present\\n\\n\")\n",
    "            f.close()\n",
    "            logging.info(f\"Validation status updated: {self.validation_config.validated_status_report_file_name}\")\n",
    "            return file_exist_status\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def number_of_columns_validation(self):\n",
    "        try:\n",
    "            vs = None\n",
    "            train_data_file = os.path.join(self.ingestion_config.unzip_data_dir_name,INGESTED_TRAIN_FILE_NAME)\n",
    "            test_data_file = os.path.join(self.ingestion_config.unzip_data_dir_name,INGESTED_TEST_FILE_NAME)\n",
    "            \n",
    "            df_train = pd.read_csv(train_data_file)\n",
    "            df_test = pd.read_csv(test_data_file)\n",
    "\n",
    "            df_train.drop(columns=self.schema.target_column,inplace=True)\n",
    "            with open(self.validation_config.validated_status_report_file_name,'a') as f:\n",
    "                f.write(f\">>>>>>>>>>>>number of column validation<<<<<<<<<<<<<\\n\\n\")\n",
    "                if df_train.shape[1] == self.schema.number_of_feature_columns:\n",
    "                        vs = True\n",
    "                        f.write(f\"Validation status:{vs}-------> Training file has {self.schema.number_of_feature_columns} columns\\n\\n\")\n",
    "                else:\n",
    "                    vs = False\n",
    "                    f.write(f\"Validation status:{vs}-------> Training file has {df_train.shape[1]} columns\\n\\n\")\n",
    "\n",
    "                if df_test.shape[1] == self.schema.number_of_feature_columns:\n",
    "                        vs = True\n",
    "                        f.write(f\"Validation status:{vs}-------> Industrial test file has {self.schema.number_of_feature_columns} columns\\n\\n\")\n",
    "                else:\n",
    "                    vs = False\n",
    "                    f.write(f\"Validation status:{vs}-------> Industrial test file has {df_train.shape[1]} columns\\n\\n\")\n",
    "            f.close()\n",
    "            logging.info(f\"Validation status updated: {self.validation_config.validated_status_report_file_name}\")\n",
    "            return vs\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def datatype_of_columns_validation(self):\n",
    "        try:\n",
    "            vs = None\n",
    "            train_data_file = os.path.join(self.ingestion_config.unzip_data_dir_name,INGESTED_TRAIN_FILE_NAME)\n",
    "            test_data_file = os.path.join(self.ingestion_config.unzip_data_dir_name,INGESTED_TEST_FILE_NAME)\n",
    "            \n",
    "            df_train = pd.read_csv(train_data_file)\n",
    "            df_test = pd.read_csv(test_data_file)\n",
    "\n",
    "            features_list = df_test.columns.to_list()\n",
    "            with open(self.validation_config.validated_status_report_file_name,'a') as f:\n",
    "                f.write(f\">>>>>>>>>>>>datatype of column validation<<<<<<<<<<<<<\\n\\n\")\n",
    "                for feature in features_list:\n",
    "                    if (df_train[feature].dtype == self.schema.datatype_of_columns[feature]) and (df_test[feature].dtype == self.schema.datatype_of_columns[feature]) :\n",
    "                            vs = True\n",
    "                            f.write(f\"Validation status:{vs}-------> The {feature} is present in Training file and Industrial test file has datatype {self.schema.datatype_of_columns[feature]} \\n\\n\")\n",
    "                    else:\n",
    "                        vs = False\n",
    "                        f.write(f\"Validation status:{vs}-------> The {feature} is not present in Training file and Industrial test file. Please check this {feature} \\n\\n\")\n",
    "                if df_train[self.schema.target_column].dtype == self.schema.datatype_of_columns[self.schema.target_column]:\n",
    "                    vs = True\n",
    "                    f.write(f\"Validation status:{vs}-------> The target column i.e {self.schema.target_column} is present in train file\\n\\n\")\n",
    "                else:\n",
    "                    vs = False\n",
    "                    f.write(f\"Validation status:{vs}-------> The target column i.e {self.schema.target_column} is not present in train file\\n\\n\")\n",
    "            f.close()\n",
    "            logging.info(f\"Validation status updated: {self.validation_config.validated_status_report_file_name}\")\n",
    "            return vs\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def null_value_of_columns_validation(self):\n",
    "        try:\n",
    "            vs = None\n",
    "            train_data_file = os.path.join(self.ingestion_config.unzip_data_dir_name,INGESTED_TRAIN_FILE_NAME)\n",
    "            test_data_file = os.path.join(self.ingestion_config.unzip_data_dir_name,INGESTED_TEST_FILE_NAME)\n",
    "            \n",
    "            df_train = pd.read_csv(train_data_file)\n",
    "            df_test = pd.read_csv(test_data_file)\n",
    "            train_status = df_train.isna().sum().sum()\n",
    "            test_status = df_test.isna().sum().sum()\n",
    "            with open(self.validation_config.validated_status_report_file_name,'a') as f:\n",
    "                f.write(f\">>>>>>>>>>>>null value of column validation<<<<<<<<<<<<<\\n\\n\")\n",
    "                if train_status == test_status == 0:\n",
    "                        vs = True\n",
    "                        f.write(f\"Validation status:{vs}-------> The is no null value in train and industrial test data\\n\\n\")\n",
    "                elif train_status != 0:\n",
    "                    vs = False\n",
    "                    null_features = [feature for feature in df_train.columns if df_train[feature].isna().sum()>0]\n",
    "                    f.write(f\"Validation status:{vs}-------> These features {null_features} have null value in train file \\n\\n\")\n",
    "                elif test_status != 0:\n",
    "                    vs = False\n",
    "                    null_features = [feature for feature in df_test.columns if df_test[feature].isna().sum()>0]\n",
    "                    f.write(f\"Validation status:{vs}-------> These features {null_features} have null value in industrial test file \\n\\n\")\n",
    "            f.close()\n",
    "            logging.info(f\"Validation status updated: {self.validation_config.validated_status_report_file_name}\")\n",
    "            return vs\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def unique_value_of_columns_validation(self):\n",
    "        try:\n",
    "            train_data_file = os.path.join(self.ingestion_config.unzip_data_dir_name,INGESTED_TRAIN_FILE_NAME)\n",
    "            test_data_file = os.path.join(self.ingestion_config.unzip_data_dir_name,INGESTED_TEST_FILE_NAME)\n",
    "            \n",
    "            df_train = pd.read_csv(train_data_file)\n",
    "            df_test = pd.read_csv(test_data_file)\n",
    "            \n",
    "            with open(self.validation_config.validated_status_report_file_name,'a') as f:\n",
    "                f.write(f\">>>>>>>>>>>>unique value of each column<<<<<<<<<<<<<\\n\\n\")\n",
    "                f.write(f\">>>>>>>>>>>>unique value of each column in train data<<<<<<<<<<<<<\\n\\n\")\n",
    "                for feature in df_train.columns:\n",
    "                        f.write(f\"{feature} has {df_train[feature].nunique()} unique values \\n\\n\")\n",
    "\n",
    "                f.write(f\">>>>>>>>>>>>unique value of each column in industrial test data<<<<<<<<<<<<<\\n\\n\")\n",
    "                for feature in df_test.columns:\n",
    "                        f.write(f\"{feature} has {df_test[feature].nunique()} unique values \\n\\n\") \n",
    "            f.close()\n",
    "            logging.info(f\"Validation status updated: {self.validation_config.validated_status_report_file_name}\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def saving_validated_data(self):\n",
    "        try:\n",
    "            train_data_file = os.path.join(self.ingestion_config.unzip_data_dir_name,INGESTED_TRAIN_FILE_NAME)\n",
    "            test_data_file = os.path.join(self.ingestion_config.unzip_data_dir_name,INGESTED_TEST_FILE_NAME)\n",
    "            \n",
    "            df_train = pd.read_csv(train_data_file)\n",
    "            df_test = pd.read_csv(test_data_file)\n",
    "\n",
    "            validated_training_data_file_path = os.path.join(self.validation_config.validated_train_dir,VALIDATED_DATA_FILE_NAME_FOR_MODEL_TRAIN)\n",
    "            validated_industrial_test_data_file_path = os.path.join(self.validation_config.validated_test_dir,VALIDATED_INDUSTRIALDATA_FILE_NAME)\n",
    "\n",
    "            df_train.to_csv(validated_training_data_file_path,index=False)\n",
    "            df_test.to_csv(validated_industrial_test_data_file_path,index=False)\n",
    "            \n",
    "            logging.info(f\"Validated train data saved in : {validated_training_data_file_path}\")\n",
    "            logging.info(f\"Validated industrial test data saved in : {validated_industrial_test_data_file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20-08-2023 22:22:04: INFO: common:  yaml file from this path config\\config.yaml read succesfully]\n",
      "[20-08-2023 22:22:04: INFO: common:  Directory already present: artifacts ]\n",
      "[20-08-2023 22:22:04: INFO: 3402709211:  Artifacts directory created at : artifacts ]\n",
      "[20-08-2023 22:22:04: INFO: common:  Directory already present: artifacts\\data_ingestion ]\n",
      "[20-08-2023 22:22:04: INFO: common:  Directory already present: artifacts\\data_ingestion\\raw_data ]\n",
      "[20-08-2023 22:22:04: INFO: common:  Directory already present: artifacts\\data_ingestion\\ingested_data ]\n",
      "[20-08-2023 22:22:04: INFO: 3402709211:  Data ingestion configuration: DataIngestionConfiguration(root_dir_name='data_ingestion', dataset_download_url='https://github.com/sumit-1492/datasets/raw/main/playground-series-s3e10.zip', zip_data_dir_name='artifacts\\\\data_ingestion\\\\raw_data', unzip_data_dir_name='artifacts\\\\data_ingestion\\\\ingested_data')]\n",
      "[20-08-2023 22:22:04: INFO: common:  Directory already present: artifacts\\data_validation ]\n",
      "[20-08-2023 22:22:04: INFO: common:  Directory already present: artifacts\\data_validation\\training_data_for_model ]\n",
      "[20-08-2023 22:22:04: INFO: common:  Directory already present: artifacts\\data_validation\\industrial_test_data ]\n",
      "[20-08-2023 22:22:04: INFO: 3402709211:  Data validation configuration: DataValidationConfiguration(validated_root_dir_name='data_validation', validated_train_dir='artifacts\\\\data_validation\\\\training_data_for_model', validated_test_dir='artifacts\\\\data_validation\\\\industrial_test_data', validated_status_report_file_name='artifacts\\\\data_validation\\\\status.txt', validated_required_files=BoxList(['train', 'test', 'sample_submission']))]\n",
      "[20-08-2023 22:22:04: INFO: common:  yaml file from this path config\\schema.yaml read succesfully]\n",
      "[20-08-2023 22:22:04: INFO: 3360515675: Validation status updated: artifacts\\data_validation\\status.txt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20-08-2023 22:22:04: INFO: 3360515675: Validation status updated: artifacts\\data_validation\\status.txt]\n",
      "[20-08-2023 22:22:05: INFO: 3360515675: Validation status updated: artifacts\\data_validation\\status.txt]\n",
      "[20-08-2023 22:22:05: INFO: 3360515675: Validation status updated: artifacts\\data_validation\\status.txt]\n",
      "[20-08-2023 22:22:06: INFO: 3360515675: Validation status updated: artifacts\\data_validation\\status.txt]\n",
      "[20-08-2023 22:22:09: INFO: 3360515675: Validated train data saved in : artifacts\\data_validation\\training_data_for_model\\pulsar.csv]\n",
      "[20-08-2023 22:22:09: INFO: 3360515675: Validated industrial test data saved in : artifacts\\data_validation\\industrial_test_data\\Industrial_pulsar_data.csv]\n"
     ]
    }
   ],
   "source": [
    "#from pulsarclassification.config.configuration import ConfigurationManager\n",
    "#from pulsarclassification.components.data_validation import DataValidation\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_configuration = config.get_data_ingestion_config()\n",
    "    data_validation_config = config.get_data_validation_configuration()\n",
    "    data_validation = DataValidation(ingestion_config=data_ingestion_configuration,\n",
    "                                     validation_config=data_validation_config)\n",
    "    data_validation.file_exist_validation()\n",
    "    data_validation.number_of_columns_validation()\n",
    "    data_validation.datatype_of_columns_validation()\n",
    "    data_validation.null_value_of_columns_validation()\n",
    "    data_validation.unique_value_of_columns_validation()\n",
    "    data_validation.saving_validated_data()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
